{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "db1b2a24-a22d-4916-81e9-6ac72da665c6",
      "cell_type": "code",
      "source": "# 1. What is Simple Linear Regression?\n\n    # Simple Linear Regression is a supervised learning algorithm used for regression tasks, where the goal is to predict a continuous value (output) from a single input (feature).\n    # It learns a linear mapping function between the input feature X and the target variable Y.\n\n        # Y=β0+β1X\n\n# How it works in\n    # 1. Training Data → You provide the algorithm with data pairs (X,Y).\n        # Example: Hours studied (X) vs Exam score (Y).\n\n    # 2. Model Fitting → The algorithm finds the best straight line by minimizing the cost function.\n        # Common cost function: Mean Squared Error (MSE)\n                J(β0,β1)=1/n∑(Yi−(β0+β1Xi))^2\n\n        # This is solved using Gradient Descent or the Normal Equation.\n\n    # 3. Prediction → Once trained, the model can predict new Y values for unseen X.\n\n# Example:\n    # Problem: Predict the price of a house based on its size (only one feature: area in sq.ft).\n    # Model: Linear regression finds a line:\n\n            # Price=β0+β1×Size\n\n    # Training: The model learns β0\t(intercept) and 𝛽1(slope).\n    # Prediction: If size = 1000 sq.ft, it predicts the corresponding price.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "128abb1c-3be4-458e-b83a-8aeba4ffd4ad",
      "cell_type": "code",
      "source": "# 2. What are the key assumptions of simple linear regression?\n`# Key Assumptions of Simple Linear Regression:\n\n    # 1. Linearity\n        # The relationship between the independent variable X and dependent variable Y is linear.\n        # That means,Y changes at a constant rate with respect to X.\n        # If the relationship is curved, linear regression won’t perform well.\n\n    # 2. Independence of Errors (No Autocorrelation)\n        # The residuals (errors) should be independent of each other.\n        # Example: In time series data, errors from one time step should not depend on previous errors.\n\n    # 3. Homoscedasticity (Constant Variance of Errors)\n        # The variance of the residuals should be constant across all levels of X.\n        # If variance increases or decreases (heteroscedasticity), predictions may be biased.\n\n    # 4. Normality of Errors\n        # The residuals (differences between observed and predicted values) should be normally distributed.\n        # This is especially important when making statistical inferences (confidence intervals, hypothesis tests).\n\n    # 5. No Perfect Multicollinearity (only in multiple regression, but worth noting)\n        # In simple linear regression (only one predictor), this isn’t an issue.\n        # But in multiple regression, predictors shouldn’t be perfectly correlated.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0d700ca-8858-4b5b-ae1c-a75b269300fa",
      "cell_type": "code",
      "source": "# 3. What does the cofficient m represent in the equation Y=mX+c?\n\n# In the linear equation:\n        # y=mX+c\n    # m = slope (coefficient)\n    # c = intercept (bias term)\n\n# Meaning of m (slope coefficient):\n    # It represents the change in y for a one-unit increase in X.\n    # In machine learning terms, m is the weight assigned to the feature X.\n    # It tells us the strength and direction of the relationship between X and y.\n\n# Interpretation:\n    # If m>0: As X increases, y increases (positive correlation).\n    # If m<0: As X increases, y decreases (negative correlation).\n    # If m=0: X has no effect on y.\n\n# Example:\n    # Suppose the model is:\n            # Score=40+5×Hours studied\n    # Here, m=5.\n    # Interpretation: For every extra 1 hour studied, the exam score increases by 5 points (on average).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b7aa8680-ad49-4799-804b-196bfacff330",
      "cell_type": "code",
      "source": "# 4. What does the intercept c represent in the equation Y=mX+c?\n    # In the linear equation:\n            # Y=mX+c\n\n    # m = slope (coefficient of X)\n    # c = intercept (bias term in ML)\n\n# Meaning of the Intercept (c):\n    # The intercept c is the value of Y when X=0.\n    # It \"anchors\" the regression line on the Y-axis.\n    # In machine learning, it is often called the bias term because it shifts the prediction up or down to better fit the data.\n\n# Interpretation:\n    # If c=10: When X=0, the model predicts Y=10.\n    # If c is large and positive → the line starts high on the Y-axis.\n    # If c is negative → the line starts below the origin on the Y-axis.\n\n# Example:\n    # Suppose the equation is:\n        # Score=40+5×Hours studied\n\n    # Here, c=40.\n    # Interpretation: If a student studies 0 hours, their predicted score is 40 marks.\n    # The model assumes some \"base score\" even without studying.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fc663d2d-28e1-4aea-b5e6-673aaf5548fc",
      "cell_type": "code",
      "source": "# 5. How do we calculate the slope m in Simple Linear Regression?\n    # We want to fit the line:\n            # Y=mX+c\n    # so that it best represents the relationship between X and Y.\n\n# Formula for the slope (m):\n    # The slope is calculated using the Least Squares Method (minimizing the squared errors between predicted and actual values).\n                # m=(∑(Xi−Xˉ)(Yi−Yˉ))/∑(Xi−Xˉ)^2\n    # Where:Xi,Yi = data points\n    # Xˉ,Yˉ = means of X and Y\n    # n = number of observations\n\n# Intuition:\n    # The numerator = covariance between X and Y.\n    # The denominator = variance of X.\n    # So, slope m is basically:\n                # m=Cov(X,Y)/Var(X)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "436ba4e2-efc8-464c-984e-d76653206961",
      "cell_type": "code",
      "source": "# 6. What is the purpose of least square method in Simple Linear Regression?\n    # Purpose of the Least Squares Method:\n        # The goal of regression is to find the best-fitting line:\n                        # Y=mX+c\n        # The least squares method ensures this line is the \"best\" by minimizing the sum of squared errors (residuals).\n\n# What are residuals?\n        # For each data point:\n            # Residual (error)=Yactual−Ypredicted\n    \t# If prediction = actual → residual = 0\n    # If prediction is wrong → residual is nonzero\n\n# What the method does:\n    # Instead of just minimizing the raw errors (which could cancel out), we square the residuals.\n    # Then we minimize the total squared error:\n            # SSE=∑(Yi−(mXi+c))^2\n    #This is called the Sum of Squared Errors (SSE).\n\n# Why square the errors?\n    # 1. Removes negatives (so errors don’t cancel out).\n    # 2. Penalizes larger errors more strongly (big mistakes matter more).\n    # 3. Makes the problem mathematically solvable using calculus.\n\n# Final Purpose:\n    # The least squares method finds the slope (m) and intercept (c) that make the line as close as possible to all the data points in terms of squared error.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0fc535a9-41ea-477e-9369-d1e895289579",
      "cell_type": "code",
      "source": "# 7. How is the coefficent of determination (R^2) interpreted in Simple Linear Regression?\n# What is R^2?\n            # R^2=1−SSres/SStot\n    # Where:\n        # SSres =∑(𝑌𝑖−𝑌^𝑖)^2 = Residual Sum of Squares (error left after regression)\n        # SStot=∑(Yi−Yˉ)2 = Total Sum of Squares (total variation in data)\n        # Y^i = predicted values\n        # Yˉ= mean of actual values\n\n# Interpretation of R^2:\n    # R^2 measures the proportion of variance in the dependent variable (Y) explained by the independent variable (X).\n        # 0≤R^2≤1\n    # R^2 =0 → Model explains none of the variance (predictions are no better than using the mean).\n        # R^2 =1 → Model explains all of the variance (perfect fit).\n    # Closer to 1 → Better fit of the regression line to the data.\n\n# Example:\n    # Suppose we predict exam scores from study hours:\n        # If R^2=0.85:\n        # = 85% of the variation in exam scores can be explained by study hours.\n        # = 15% is due to other factors (like sleep, teaching quality, natural ability, etc.).\n        # If R^2=0.2:\n        # = Only 20% of the variation is explained by study hours.\n        # = Model is weak and not very predictive.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74d2c85f-1aad-475f-8710-134ff91b5fd0",
      "cell_type": "code",
      "source": "# 8. What is Multiple Linear Regression?\n# Multiple Linear Regression is a supervised learning algorithm (regression) that models the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, …, Xₙ).\n# It’s an extension of simple linear regression.\n\n# General Equation:\n                # Y=β0+β1X1+β2X+⋯+βnXn+ϵ\n    # Where:\n        # Y = dependent variable (target)\n        # β0 = intercept (bias term)\n        # β1,β2,…,βn = coefficients (slopes/weights for each feature)\n        # X1,X2,…,Xn = independent variables (features)\n        # ϵ = error term\n\n3 Example:\n    # Suppose we want to predict a house price (Y) based on:\n        # X1 = house size (sq.ft)\n        # X2 = number of bedrooms\n        # X3 = distance to city center\n    # Model might look like:\n    # Price=50,000+200×(Size)+10,000×(Bedrooms)−5,000×(Distance)\n    # Each coefficient (β) tells us how much Y changes for a one-unit change in that variable, holding others constant.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eb89d385-514f-4050-a377-88453c37dfb1",
      "cell_type": "code",
      "source": "# 9. What is the main difference between Simple and Multiple Linear Regression?\n\n# In Simple Linear Regression, we have just one independent variable (X) used to predict the dependent variable (Y). The relationship is represented by a straight line in two dimensions. For example, predicting a student’s exam score based only on the number of hours studied. The model looks like:\n    # Y=β0+β1X+ϵ\n    # Here, the slope (β1) tells us how much Y changes when X increases by one unit.\n\n# In Multiple Linear Regression, we use two or more independent variables to predict the dependent variable. Instead of a line, the best fit becomes a plane (or hyperplane if more than two predictors). For example, predicting exam score not just from study hours, but also from hours of sleep and number of practice tests taken. The model looks like:\n    # Y=β0+β1X1+β2X2+β3X3+⋯+ϵ\n    # Each coefficient (βi) shows the effect of that predictor on Y, while keeping all the other predictors constant.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c4aa0d1d-b4d4-4075-838b-1c4e0e2cfb5a",
      "cell_type": "code",
      "source": "# 10. What are the key assumptions of Multiple Linear Regression?\n\n# Key Assumptions of Multiple Linear Regression\n    # 1. Linearity\n        # The relationship between the dependent variable Y and each independent variable Xi is assumed to be linear.\n        # If the relationship is curved or nonlinear, linear regression won’t capture it well.\n\n    # 2. Independence of Errors (No Autocorrelation)\n        # The residuals (errors) should be independent of each other.\n        # In time series data, this means one error should not depend on the previous one.\n\n    # 3. Homoscedasticity (Constant Variance of Errors)\n        # The residuals should have constant variance across all levels of the independent variables.\n        # If variance changes (heteroscedasticity), predictions may be unreliable.\n\n    # 4. Normality of Errors\n        # The residuals should be approximately normally distributed.\n        # This is especially important for hypothesis testing and confidence intervals.\n\n    # 5. No Perfect Multicollinearity\n        # Independent variables should not be highly correlated with each other.\n        # High multicollinearity makes it difficult to separate the individual effect of each predictor.\n\n    # 6. Independence of Observations\n        # Each observation (data point) should be independent of others.\n        # Example: data from the same person measured multiple times may violate this.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1694adad-c4e3-4a0e-a0f5-f00aa00a1220",
      "cell_type": "code",
      "source": "# 11. What is heteroscedasticity, and how does it affect the result of a Multiple Linear Regression model?\n\n# In regression models, heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables.\n    # Homoscedasticity = residuals have constant spread (good).\n    # Heteroscedasticity = residuals have uneven spread, often forming a \"funnel shape\" in residual plots.\n\n# Mathematically:\n    # Homoscedasticity - Var(ϵ∣X)=σ^2 (constant).\n    # Heteroscedasticity - Var(ϵ∣X)not=σ^2 (changes with X).\n\n# How it Affects Multiple Linear Regression\n    # 1. Coefficients (β) are still unbiased\n        # The model can still estimate slopes and intercept correctly on average.\n\n    # 2. Standard errors become unreliable\n        # Estimated standard errors of coefficients are wrong.\n        # This makes t-tests, F-tests, and confidence intervals invalid.\n\n    # 3. Model inference becomes misleading\n        # You might wrongly conclude a predictor is significant (or not significant).\n\n    # 4. Predictions may be less efficient\n        # The model does not achieve the \"Best Linear Unbiased Estimator (BLUE)\" property anymore.\n        # In other words, predictions are unbiased but not the most precise.\n\n# Example (Intuition)\n    # Suppose you’re predicting house prices from house size.\n        # For small houses, residuals (errors) are small.\n        # For large houses, residuals get bigger (more variability in price).\n    # This funnel effect means heteroscedasticity exists → leading to unreliable hypothesis tests about your predictors.\n\n# How to Detect Heteroscedasticity\n    # Residual plot → plot residuals vs predicted values. A \"fan\" or \"cone\" shape suggests heteroscedasticity.\n    # Statistical tests → Breusch–Pagan test, White test.\n\n# How to Fix It\n    # Transform the dependent variable (e.g., log transformation).\n    # Use Weighted Least Squares (WLS) instead of ordinary least squares.\n    # Use robust standard errors (e.g., White’s heteroscedasticity-consistent SE).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e0a454f1-ffc3-4ff0-b360-47c8e8d8c6f1",
      "cell_type": "code",
      "source": "# 12. How can you improve a Multiple Liear Regression model with high multicollinearity?\n\n# 1. Remove Highly Correlated Predictors\n    # Drop one of the variables that are strongly correlated with each other (e.g., if X1 and X2 have correlation > 0.9, keep only one).\n    # This reduces redundancy in predictors.\n\n# 2. Use Dimensionality Reduction Techniques\n    # Apply Principal Component Analysis (PCA) or Factor Analysis to combine correlated variables into uncorrelated components.\n    # This way, the model uses fewer independent but informative predictors.\n\n# 3. Regularization Methods\n    # Instead of ordinary least squares, use:\n    # Ridge Regression (L2 regularization): shrinks coefficients of correlated predictors but keeps them in the model.\n    # Lasso Regression (L1 regularization): can shrink some coefficients to zero, effectively selecting important variables.\n    # Elastic Net: combines both Ridge and Lasso.\n\n# 4. Centering or Standardizing Variables\n    # Mean-centering or scaling predictors can sometimes reduce multicollinearity, especially when interaction or polynomial terms are included.\n\n# 5. Increase Sample Size\n    # If feasible, collecting more data can stabilize coefficient estimates and lessen the harmful effects of multicollinearity.\n\n# 6. Domain Knowledge Variable Selection\n    # Instead of blindly including all correlated predictors, use domain expertise to choose the most meaningful variables.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9dea3744-d718-4986-affc-34cf6912d451",
      "cell_type": "code",
      "source": "# 13. What are some common techniques for transforming categorical variables for use in regression model?\n\n# Common Techniques to Transform Categorical Variables\n    # 1. Label Encoding\n        # Assigns a unique integer to each category.\n        # Example: Color = {Red=0, Green=1, Blue=2}\n        # Works well for ordinal data, but for nominal data it can mislead the model (since it imposes order).\n\n    # 2. One-Hot Encoding\n        # Creates binary (0/1) dummy variables for each category.\n        # Example: Color = Red → [1,0,0], Green → [0,1,0], Blue → [0,0,1]\n        # Good for nominal data.\n        # Can cause high dimensionality if there are many categories.\n\n    # 3. Ordinal Encoding\n        # Explicitly assign ordered integers based on category ranking.\n        # Example: Size = {Small=1, Medium=2, Large=3}\n        # Useful when categories have a natural order.\n\n    # 4. Target Encoding (Mean Encoding)\n        # Replace each category with the mean of the target variable for that category.\n        # Example: if predicting House Price, encode Neighborhood as the average house price in that neighborhood.\n        # Risk: can cause data leakage if not done carefully (use cross-validation).\n\n    # 5. Frequency / Count Encoding\n        # Replace each category with its frequency (count) in the dataset.\n        # Example: Category A → 120, Category B → 50, Category C → 30.\n        # Keeps dimensionality low compared to one-hot.\n\n    # 6. Binary Encoding\n        # Categories are converted into binary digits and split across multiple columns.\n        # Example: Category 1 → 001, Category 2 → 010, Category 3 → 011.\n        # Useful for high-cardinality categorical variables.\n\n    # 7. Embedding Representations (Advanced ML / Deep Learning)\n        # Learn dense vector embeddings for categories (similar to word embeddings in NLP).\n        # Particularly useful for large datasets and high-cardinality features.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0535d4ec-5628-4bcb-af1f-cd6d7e11cec9",
      "cell_type": "code",
      "source": "# 14. What is the role of interaction terms in Multiple Linear Regression?\n\n# 1. Capture Combined Effects\n    # An interaction term models the joint effect of two (or more) predictors on the dependent variable.\n    # Example: If you are modeling Salary = β0 + β1(Education) + β2(Experience) + β3(Education × Experience)\n    # The interaction Education × Experience means the effect of education on salary depends on the level of experience.\n\n# 2. Improve Model Accuracy\n    # Without interaction terms, the model might miss important relationships.\n    # Including them helps better fit real-world data where predictors work together, not separately.\n\n# 3. Handle Non-Additive Relationships\n    # Linear regression assumes additivity:\n        # Y = β0 + β1X1 + β2X2\n    # With interaction:\n        # Y = β0 + β1X1 + β2X2 + β3(X1 × X2)\n    # The coefficient β3 shows how much the effect of X1 changes when X2 increases by one unit (and vice versa).\n\n# 4. Interpretation Insight\n    # If β3 ≠ 0 (significant), it means the predictors don’t just add up — they interact.\n    # This gives deeper insights into relationships between features.\n\n# 5. Machine Learning Perspective\n    # In linear models, interaction terms must be manually added (feature engineering).\n    # In tree-based models (Decision Trees, Random Forest, Gradient Boosting), interactions are learned automatically by splitting on multiple features.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a02113dd-40d4-4a33-94d2-fbf01df47824",
      "cell_type": "code",
      "source": "# 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n\n# In Simple Linear Regression (SLR)\n    # Equation:\n            # Y=β0+β1X+ε\n    # Intercept (β₀): The predicted value of the dependent variable Y when the independent variable X = 0.\n    # Example: If we predict Salary = β₀ + β₁(Education Years),\n        # β₀ is the expected salary when Education = 0.\n\n    # Interpretation is usually straightforward because only one predictor exists.\n\n# In Multiple Linear Regression (MLR)\n    # Equation:\n            # Y=β0+β1X1+β2X2+...+βkXk+ε\n    # Intercept (β₀): The predicted value of Y when all independent variables (X₁, X₂, …, Xₖ) = 0.\n    # Example: If predicting House Price = β₀ + β₁(Size) + β₂(Location Score) + β₃(Age),\n        # β₀ represents the predicted house price when Size = 0, Location Score = 0, and Age = 0.\n    # This may not always be meaningful in practice (e.g., a house of size 0 sq. ft. doesn’t exist). In such cases, the intercept is just a mathematical adjustment for the regression line/plane.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8f284a89-7951-492c-b95a-d207f807fd88",
      "cell_type": "code",
      "source": "# 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n\n# 1. Represents the Relationship\n    # In regression, the slope (β) measures the change in the dependent variable (Y) for a one-unit change in the independent variable (X), while keeping all other predictors constant (in multiple regression).\n    # It tells us the direction (positive/negative) and strength (magnitude) of the relationship.\n\n# 2. Simple Linear Regression (SLR)\n    # Equation:\n            # Y=β0+β1X+ε\n\n    # Slope (β₁):\n        # If β₁ = 5 → For every 1-unit increase in X, Y increases by 5 units (on average).\n        # If β₁ = -3 → For every 1-unit increase in X, Y decreases by 3 units.\n    # Interpretation is straightforward since there’s only one predictor.\n\n# 3. Multiple Linear Regression (MLR)\n    # Equation:\n            # Y=β0+β1X1+β2X2+...+βkXk+ε\n    # Slope (βᵢ):\n        # Represents the expected change in Y for a 1-unit increase in Xᵢ, holding all other variables constant.\n    # Example: If predicting Salary = β₀ + β₁(Education) + β₂(Experience),\n        # β₁ = 2000 → Each extra year of education increases salary by $2000, assuming experience is fixed.\n\n# 4. Effect on Predictions\n    # Slopes directly control the prediction values:\n        # A larger slope → stronger effect of that predictor on Y.\n        # A slope close to 0 → weak or negligible effect.\n    # If slopes are wrongly estimated (e.g., due to multicollinearity or bias), predictions will be unreliable.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4cc767ca-77af-48d0-8f08-65311dc11aab",
      "cell_type": "code",
      "source": "# 17. How does the intercept in a regression model provide context for the relationship between variables?\n\n# 1. Baseline Value of Y\n    # The intercept represents the predicted value of the dependent variable (Y) when all independent variables (X’s) are equal to zero.\n    # It acts as the starting point (baseline) of the regression line or plane.\n\n# 2. Provides Context for Slopes\n    # Slopes (β’s) tell us how Y changes when predictors change, but without the intercept, we don’t know the baseline level of Y.\n    # The intercept anchors the regression line so that slopes can describe changes relative to that baseline.\n\n# 3. Interpretation Depends on Variables\n    # In Simple Linear Regression (SLR):\n        # Intercept = predicted Y when X = 0.\n        # Example: Predicting Salary = β₀ + β₁(Education Years) → β₀ = predicted salary of a person with 0 years of education.\n    # In Multiple Linear Regression (MLR):\n        # Intercept = predicted Y when all predictors = 0.\n        # Example: House Price = β₀ + β₁(Size) + β₂(Location Score) → β₀ = predicted price when Size=0 and Location Score=0.\n        # Sometimes, this situation isn’t realistic (a house of size 0), so the intercept may not have direct meaning but still mathematically centers the model.\n\n# 4. Helps Adjust Predictions\n    # The intercept adjusts the model so predictions are close to the actual mean of Y.\n    # Without it, the regression line might be forced through the origin (0,0), which usually makes predictions worse unless it’s logically required.\n\n# 5. Machine Learning Perspective\n    # In ML, the intercept is still crucial:\n    # It helps the model shift predictions up or down to minimize error.\n    # Example: In scikit-learn’s LinearRegression, fit_intercept=True ensures the line isn’t forced through zero unless specified.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4b172eeb-a947-40a5-a86c-01cdf491992a",
      "cell_type": "code",
      "source": "# 18. What are the limitations of using R^2 as a sole measure of model performance?\n\n# Limitations of using R² as the sole performance metric in Machine Learning:\n    # 1. Overfitting Risk – R² always increases when more features are added, even if they are irrelevant. In ML, this can give a false sense of improvement in high-dimensional datasets.\n    # 2.Not Robust to Nonlinear Models – Many ML models (e.g., decision trees, random forests, neural networks) capture nonlinear patterns. R² may undervalue their performance if the relationship is complex.\n    # 3. Poor Generalization Indicator – R² reflects fit on training data, but does not guarantee good test performance. Cross-validation or test set evaluation is more reliable in ML.\n    # 4. Insensitive to Scale of Errors – R² doesn’t directly tell you how large the prediction errors are (MAE, RMSE are better for that).\n    # 5. Not Suitable for Classification Problems – R² only applies to regression, so in ML workflows that include both regression and classification, it’s not a universal metric.\n    # 6. Cannot Handle Imbalanced or Skewed Data Well – In datasets with high variance in the target, R² might look good even if the model performs poorly on important subgroups.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "00b10e19-8046-4868-a4ea-d938f95b427c",
      "cell_type": "code",
      "source": "# 19. How would you interpret a large standard error for a regression coefficient?\n\n# Interpretation of a Large Standard Error (SE) for a Regression Coefficient:\n    # 1. High Uncertainty in Estimate\n            # A large SE means the estimated coefficient is unstable and may vary significantly if you collect another sample.\n\n    # 2. Low Precision\n            # The model is not confident about the true effect size of that predictor on the target variable.\n\n    # 3. Possible Insignificance of Predictor\n            # If SE is large relative to the coefficient value, the corresponding t-statistic will be small → leading to a high p-value, suggesting the predictor might not be statistically significant.\n\n    # 4. Potential Issues in the Model\n            # Large SEs often indicate:\n            # Multicollinearity (predictors highly correlated)\n            # Small sample size\n            # High variability in data\n            # Model misspecification",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "396b0dba-65c0-4797-b5c3-3d24148f2126",
      "cell_type": "code",
      "source": "# 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n\n# Homoscedasticity (good) → residuals are randomly scattered around zero with roughly constant spread.\n\n# Heteroscedasticity (problem) → residuals show patterns like:\n    # Funnel shape (spread increasing or decreasing with fitted values).\n    # Curved or systematic patterns in variance.\n    # “Fan-out” or “cone” shape as predictions increase.\n\n# Why is it important to address?\n    # 1. Invalid Statistical Inference\n        # Heteroscedasticity violates regression assumptions → standard errors of coefficients become biased.\n        # This leads to unreliable t-tests and p-values, possibly making insignificant features look important (or vice versa).\n\n    # 2. Poor Model Generalization\n        # In ML, if variance is not constant, the model may perform poorly on new data. Predictions could be less reliable for certain ranges of input values.\n\n    # 3. Impacts Model Interpretation\n        # Coefficient estimates remain unbiased, but they are no longer efficient (not minimum variance).\n\n# Remedies\n    # Transformations (log, square root, Box-Cox) to stabilize variance.\n    # Weighted Least Squares to give less weight to high-variance observations.\n    # Robust Standard Errors (e.g., White’s correction).\n    # In ML practice → models like tree-based methods (Random Forest, Gradient Boosting) are more robust to heteroscedasticity.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "768e1d29-42ed-4345-bb46-06367e427073",
      "cell_type": "code",
      "source": "# 21. What does it mean if a Multiple Liear Regression model has a high R^2 but low adjusted R^2?\n\n# In machine learning, a high R² means the model appears to fit the training data well.\n# But a low Adjusted R² indicates that many features are not contributing useful information. It penalizes unnecessary complexity.\n# The model may be overfitting because it has too many irrelevant or redundant features.\n# It suggests poor feature selection — the model is capturing noise instead of true signal.\n# This highlights the need for regularization (like Lasso or Ridge) or feature selection techniques to reduce dimensionality and keep only important predictors.\n\n# Example:\n# Suppose you’re predicting car prices using 200 features, but only 10 are truly important.\n    # R² might stay high since irrelevant features inflate the apparent fit.\n    # Adjusted R² will drop, warning you that the added features don’t improve generalization.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "411e8c62-8c13-43ab-b4a4-aac80119e8e0",
      "cell_type": "code",
      "source": "# 22. Why is it important to scale variables in Multiple Linea Regression?\n\n# Scaling (standardization or normalization) ensures that all features contribute fairly to the model and improves numerical stability.\n\n# Reasons why scaling is important:\n    # 1. Coefficient comparability –\n        # In regression, coefficients represent the effect of one unit change in a feature. If features are on very different scales (e.g., \"salary\" in lakhs vs. \"age\" in years), the larger-scaled variable can dominate, making it hard to interpret coefficients.\n\n    # 2. Numerical stability –\n        # Regression uses matrix operations (like inverting X^TX). If variables have very different magnitudes, it can cause large condition numbers → unstable computations → poor estimates of coefficients.\n\n    # 3. Gradient-based optimization (ML context) –\n        # When regression is solved using gradient descent (common in ML libraries), unscaled features make gradients uneven, slowing down convergence. Scaling leads to faster and more reliable optimization.\n\n    # 4. Regularization impact (Ridge, Lasso, ElasticNet) –\n        # Without scaling, regularization penalties (L1/L2) will unfairly shrink coefficients of variables with larger scales. Scaling ensures fair penalization.\n\n# Example:\n    # Predicting house price with features:\n    # \"Area\" = 2000 sqft\n    # \"Number of rooms\" = 5\n    # If not scaled, \"Area\" dominates purely because of its magnitude. After scaling (e.g., z-score normalization), both variables contribute proportionately.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "67d58468-a38a-4136-b321-856223ed159b",
      "cell_type": "code",
      "source": "# 23. What is polynomial regression?\n\n# Polynomial Regression is an extension of Linear Regression where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial.\n# Instead of fitting a straight line:\n        # y=β0+β1x+ϵ\n\n# Polynomial regression fits a curve like:\n        # y=β0+β1x+β2x^2+β3x^3+⋯+βnx^n+ϵ\n# Key Points\n    # 1. Still linear in parameters –\n        # Even though the curve is nonlinear in x, the model is linear in the coefficients (β), so it’s solved with linear regression methods.\n\n    # 2. Captures nonlinear relationships –\n        # Useful when data shows a curved trend that straight-line regression cannot capture.\n\n    # 3. Feature engineering approach –\n        # Polynomial regression is essentially creating new features (like x^2,x^3,…) and running linear regression on them.\n\n    # 4. Risk of overfitting –\n        # Higher-degree polynomials can fit training data too closely, leading to poor generalization. Regularization or cross-validation is often used to pick the right degree.\n\n    # 5. Relation to ML models –\n        # Polynomial regression is a simple way to introduce nonlinearity, but in ML, it is often replaced by more flexible models like Decision Trees, Random Forests, or Neural Networks.\n\n# Example:\n    # If you want to predict car price based on age of the car:\n    # Linear regression might underestimate the dip after a certain age.\n    # Polynomial regression (say degree 2 or 3) can model that curve better.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "154047f2-9901-4d7d-a762-c98874e5750c",
      "cell_type": "code",
      "source": "# 24. How does polynomial regression differ from linear regression?\n\n# 1. Nature of Relationship\n    # Linear Regression: Models a straight-line relationship between independent and dependent variables.\n            # y=β0+β1x\n    # Polynomial Regression: Models a curved (nonlinear) relationship by including higher-order powers of the predictors.\n            # y=β0+β1x+β2x^2+⋯+βnx^n\n\n# 2. Complexity\n    # Linear regression assumes a linear trend.\n    # Polynomial regression increases flexibility to capture nonlinear patterns.\n\n# 3. Interpretability\n    # Linear regression coefficients are easy to interpret (slope & intercept).\n    # Polynomial regression coefficients are harder to interpret directly since they involve squared, cubic, etc., terms.\n\n# 4. Overfitting Risk\n    # Linear regression is less flexible, usually underfits complex data.\n    # Polynomial regression, especially with higher degrees, may overfit the training data.\n\n# Example:\n    # Predicting house price with respect to square footage:\n    # Linear regression assumes price increases at a constant rate per sqft.\n    # Polynomial regression allows price to increase at varying rates (e.g., initially sharp increase, then flattening out).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "00117819-5efc-428b-950f-33101ecb8a75",
      "cell_type": "code",
      "source": "# 25. When is polynomial regression used?\n\n# Polynomial regression is used when the relationship between independent and dependent variables is nonlinear but can be approximated well by a polynomial function.\n\n# Situations where it’s useful:\n    # 1. Nonlinear trends –\n        # When data clearly shows a curved pattern that linear regression cannot capture (e.g., quadratic or cubic growth/decline).\n\n    # 2. Feature engineering –\n        # To enrich linear models by creating polynomial terms (e.g., 𝑥^2,𝑥^3) instead of switching to more complex nonlinear models.\n\n    # 3. Low to moderate complexity problems –\n        # When the data is not too large or noisy, and simple polynomial features can model the pattern without requiring advanced ML models like trees or neural nets.\n\n    # 4. Scientific/engineering relationships –\n        # Many natural phenomena (e.g., projectile motion, population growth, chemical reactions) follow polynomial-like curves.\n\n# Examples:\n    # Predicting house prices vs. area where price increases but starts to plateau (quadratic pattern).\n    # Modeling learning curves (initial rapid improvement, then slower growth).\n    # Physics experiments, e.g., distance traveled vs. time under constant acceleration → quadratic relationship.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cfcf7a5d-afa2-493a-b486-288b687afff2",
      "cell_type": "code",
      "source": "# 26. What is the general equation for polynomial regression?\n\n# The general form of a polynomial regression model of degree n is:\n        # y=β0+β1x+β2x^2+β3x^3+⋯+βnx^n+ϵ\n    # Where:\n    # y = dependent (target) variable\n    # x = independent (predictor) variable\n    # β0,β1,…,βn = regression coefficients\n    # n = degree of the polynomial\n    # ϵ = error term\n\n# Key Notes (ML perspective):\n    # 1. It is linear in coefficients (β), even though the relationship between x and y is nonlinear.\n    # 2. For multiple features, polynomial regression includes cross-terms as well, e.g.:\n            # y=β0+β1x1+β2x2+β3x^2+β4x1x2+β5x^2+⋯+ϵ\n    # 3. Polynomial regression is basically linear regression applied to polynomially transformed features.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4f6470e0-50b7-41a0-b504-309628e7cb21",
      "cell_type": "code",
      "source": "# 27. Can polynomial regression be applied to multiple variables?\n\n# This is often referred to as multivariate polynomial regression (not to be confused with multiple linear regression). In this case, instead of just adding higher-degree powers of a single predictor, you expand the model to include polynomial terms of multiple predictors and their interactions.\n\n# Example:\n    # Suppose you have two variables x1 and x2.\n    # A second-degree polynomial regression model could look like:\n            # y=β0+β1x1+β2x2+β3x^2+β4x^2+β5x1x2+ϵ\n    # x1,x2 → linear terms\n    # x^2,x^2 → squared terms (nonlinear effects)\n    # x1x2 → interaction term",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d7ef982c-36f6-4e8d-b014-80317885af94",
      "cell_type": "code",
      "source": "# 28. What are the limitations of polynomial regression?\n\n# Limitations of Polynomial Regression\n\n    # 1. Overfitting\n        # Higher-degree polynomials can fit the training data extremely well but fail to generalize to new data.\n        # This leads to high variance and poor predictive performance.\n\n    # 2. Model Complexity\n        # As the polynomial degree or number of variables increases, the number of terms grows rapidly.\n        # For example, with many predictors, a 3rd-degree polynomial can generate hundreds or thousands of features.\n\n    # 3. Extrapolation Issues\n        # Polynomial curves behave unpredictably outside the range of training data (e.g., values shoot up or down sharply).\n        # This makes them unreliable for forecasting beyond observed data.\n\n    # 4. Multicollinearity\n        # Polynomial features (e.g., x,x^2,x^3) are often highly correlated.\n        # This can make coefficient estimates unstable and difficult to interpret.\n\n    # 5. Interpretability\n        # Unlike linear regression, where coefficients have clear meanings, polynomial coefficients are less intuitive and harder to explain.\n\n    # 6. Computational Cost\n        # For high-dimensional data, polynomial expansion can become computationally expensive and memory-heavy.\n\n    # 7. Not Always the Best Fit\n        # Many real-world nonlinear relationships are not polynomial in nature.\n        # Alternatives like decision trees, random forests, or kernel methods often model nonlinearity more effectively.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ef65d4a1-2994-4d48-b77f-1228ed61882f",
      "cell_type": "code",
      "source": "# 29. What methods can be used to evaluate mode fit when selecting the degree of polynomial?\n\n# 1. Visual Inspection of Residuals\n    # Plot residuals (errors) against predicted values.\n    # If residuals show patterns (e.g., curves), a higher-degree polynomial may be needed.\n    # Ideally, residuals should look random (no clear pattern).\n\n# 2. Train-Test Split (Holdout Validation)\n    # Split the dataset into training and test sets.\n    # Fit polynomials of different degrees and compare test set performance.\n    # Select the degree that minimizes test error.\n\n# 3. Cross-Validation (Preferred Method)\n    # Use k-fold cross-validation to evaluate how each polynomial degree generalizes.\n    # This avoids relying on a single train-test split.\n    # Choose the degree with the lowest average cross-validation error.\n\n# 4. Information Criteria\n    # Use metrics that penalize model complexity:\n    # AIC (Akaike Information Criterion)\n    # BIC (Bayesian Information Criterion)\n    # Lower values suggest a better trade-off between fit and complexity.\n\n# 5. Goodness-of-Fit Metrics\n    # Compare polynomial degrees using metrics such as:\n    # R² and Adjusted R² (Adjusted R² is better since it accounts for extra features).\n    # RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error) on validation data.\n\n# 6. Regularization-Based Selection\n    # Instead of manually picking degree, use regularization techniques (Ridge, Lasso) with polynomial features.\n    # Regularization shrinks unnecessary higher-degree terms, effectively selecting an optimal fit automatically.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a9c6e17-bc2d-40b5-85f5-10f2914af5a8",
      "cell_type": "code",
      "source": "# 30. Why is visualization important in polynomial regression?\n\n# Reasons why visualization is important in polynomial regression\n\n# 1. Understanding the Fit\n    # Visualizing the regression curve against the data points helps you see whether the polynomial captures the underlying trend or is under/overfitting.\n    \n# 2. Detecting Overfitting / Underfitting\n    # A very high-degree polynomial may fit training data too closely (wavy curve), while a low-degree polynomial may miss important patterns.\n    # A plot makes these issues visible immediately.\n\n# 3. Residual Analysis\n    # Residual plots show whether errors are randomly distributed or still contain patterns.\n    # If residuals show curves, the degree may need adjustment.\n\n# 4. Interpreting Nonlinear Relationships\n    # Polynomial regression can create complex curves that are hard to interpret numerically.\n    # Visualization makes the nonlinear relationship between variables and target more intuitive.\n\n# 5. Extrapolation Awareness\n    # Polynomial models can behave wildly outside the data range (e.g., curve shoots upward or downward).\n    # Plotting highlights these risks so you don’t mistakenly trust predictions far beyond the observed data.\n\n# 6. Model Comparison\n    # By plotting polynomials of different degrees, you can visually compare which one balances smoothness and accuracy.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bd8ad204-5161-47ab-80f5-4be8b8c9f5e3",
      "cell_type": "code",
      "source": "# 31. How is polynomial regression implemented in Python?\n\n# Steps to Implement Polynomial Regression in Python\n    # 1. Import Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n    # 2. Create Sample Data\nX = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\ny = np.array([1.2, 1.9, 3.2, 4.8, 8.5, 11.3])\n\n    # 3. Transform Features into Polynomial Features\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n    # 4. Fit Linear Regression on Polynomial Features\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\n    # 5. Make Predictions\ny_pred = model.predict(X_poly)\n\n    # 6. Visualization\nplt.scatter(X, y, color='blue', label='Actual Data')\nplt.plot(X, y_pred, color='red', label='Polynomial Fit (degree=2)')\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}